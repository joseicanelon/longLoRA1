{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Written by Yukang Chen<br>\n", "Core code based on https://github.com/CStanKonrad/long_llama<br>\n", "<br>\n", "Licensed under the Apache License, Version 2.0 (the \"License\");<br>\n", "you may not use this file except in compliance with the License.<br>\n", "You may obtain a copy of the License at<br>\n", "<br>\n", "    http://www.apache.org/licenses/LICENSE-2.0<br>\n", "<br>\n", "Unless required by applicable law or agreed to in writing, software<br>\n", "distributed under the License is distributed on an \"AS IS\" BASIS,<br>\n", "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>\n", "See the License for the specific language governing permissions and<br>\n", "limitations under the License."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import math\n", "import torch\n", "import argparse\n", "import random\n", "import numpy as np\n", "from numpy import random\n", "from tqdm import tqdm\n", "import transformers\n", "from peft import PeftModel\n", "from llama_attn_replace import replace_llama_attn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def parse_config():\n", "    parser = argparse.ArgumentParser(description='arg parser')\n", "    parser.add_argument('--base_model', type=str, default=\"/data1/pretrained-models/llama-7b-hf\")\n", "    parser.add_argument('--cache_dir', type=str, default=\"./cache\")\n", "    parser.add_argument('--context_size', type=int, default=-1, help='context size during fine-tuning')\n", "    parser.add_argument('--flash_attn', type=bool, default=True, help='whether to use flash attention 2')\n", "    parser.add_argument('--max_tokens', type=int, default=32000, help='maximum token length for evaluation')\n", "    parser.add_argument('--interval', type=int, default=1000, help='interval for evaluation')\n", "    parser.add_argument('--num_tests', type=int, default=10, help='number of repeat testing for each length')\n", "    args = parser.parse_args()\n", "    return args"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generate_prompt_landmark(n_garbage, seed):\n", "    \"\"\"Generates a text file and inserts an passkey at a random position.\"\"\"\n", "    rnd_state = random.get_state()\n", "    random.seed(seed)\n", "    n_garbage_prefix = random.randint(0, n_garbage)\n", "    n_garbage_suffix = n_garbage - n_garbage_prefix\n", "    task_description = \"There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\"\n", "    garbage = \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\"\n", "    garbage_inf = \" \".join([garbage] * 5000)\n", "    assert len(garbage_inf) >= n_garbage\n", "    garbage_prefix = garbage_inf[:n_garbage_prefix]\n", "    garbage_suffix = garbage_inf[:n_garbage_suffix]\n", "    pass_key = random.randint(1, 50000)\n", "    information_line = f\"The pass key is {pass_key}. Remember it. {pass_key} is the pass key.\"\n", "    final_question = \"What is the pass key? The pass key is\"\n", "    lines = [\n", "        task_description,\n", "        garbage_prefix,\n", "        information_line,\n", "        garbage_suffix,\n", "        final_question,\n", "    ]\n", "    random.set_state(rnd_state)\n", "    return \"\\n\".join(lines), str(pass_key)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def passkey_retrieval_test(model, tokenizer, device, use_cache=False, n_garbage=60000, seed=666):\n", "    prompt, answer = generate_prompt_landmark(n_garbage, seed)\n", "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n", "    input_ids = input_ids.to(device)\n", "    len_token = input_ids.shape[-1]\n", "    answer_ids = tokenizer(answer, return_tensors=\"pt\").input_ids[:, 1:] # drop BOS\n", "    generation_output = model.generate(\n", "        input_ids=input_ids, max_new_tokens=answer_ids.shape[-1], num_beams=1, use_cache=use_cache\n", "    )\n", "    model_answer = generation_output[0, -answer_ids.shape[-1]:].cpu()\n", "    is_correct = (model_answer == answer_ids[0]).all().item()\n", "    #print(f\"The correct answer is {tokenizer.decode(answer_ids[0].cpu())}\")\n", "    #print(f\"The model answer is {tokenizer.decode(model_answer.cpu())}, is_correct : {is_correct}\")\n", "    return is_correct, len_token"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main(args):\n", "    device = \"cuda:0\"\n", "    torch.cuda.set_device(device)\n", "    print(\"base model\", args.base_model)\n", "    if args.flash_attn:\n", "        replace_llama_attn(use_full=True)\n\n", "    # Set RoPE scaling factor\n", "    config = transformers.AutoConfig.from_pretrained(\n", "        args.base_model,\n", "        cache_dir=args.cache_dir,\n", "    )\n", "    context_size = args.context_size\n", "    orig_ctx_len = getattr(config, \"max_position_embeddings\", None) # this value should be 4096 for LLaMA2 models\n", "    if orig_ctx_len and context_size > orig_ctx_len:\n", "        scaling_factor = float(math.ceil(context_size / orig_ctx_len))\n", "        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n\n", "    # Load model and tokenizer\n", "    model = transformers.AutoModelForCausalLM.from_pretrained(\n", "        args.base_model,\n", "        config=config,\n", "        cache_dir=args.cache_dir,\n", "        torch_dtype=torch.float16,\n", "        device_map=\"auto\",\n", "    )\n", "    model.resize_token_embeddings(32001)\n", "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n", "        args.base_model,\n", "        cache_dir=args.cache_dir,\n", "        model_max_length=args.context_size if args.context_size > orig_ctx_len else orig_ctx_len,\n", "        padding_side=\"right\",\n", "        use_fast=False,\n", "    )\n", "    total_test_points = args.max_tokens // args.interval\n", "    all_accuries = {}\n", "    for i in range(total_test_points):\n", "        # This is a rough ratio to control the number of texts and tokens\n", "        n_garbage = int(3.75 * (i + 1) * args.interval // 1024 * 1024)\n", "        passed_tests = 0\n", "        total_tokens = 0\n", "        for i in range(args.num_tests):\n", "            is_correct, len_tokens = passkey_retrieval_test(model, tokenizer, device, use_cache=not args.flash_attn, n_garbage=n_garbage, seed=i)\n", "            passed_tests += is_correct\n", "            total_tokens += len_tokens\n", "        avg_tokens = total_tokens//args.num_tests\n", "        accuracy = float(passed_tests)/args.num_tests\n", "        print(\"accuracy on the token length %d is %f\"%(avg_tokens, accuracy))\n", "        all_accuries[str(avg_tokens)] = accuracy\n", "    print(\"accuries over tokens\", all_accuries)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    args = parse_config()\n", "    main(args)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}