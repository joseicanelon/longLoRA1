{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Written by Yukang Chen<br>\n", "Some code based on https://github.com/epfml/landmark-attention<br>\n", "<br>\n", "Licensed under the Apache License, Version 2.0 (the \"License\");<br>\n", "you may not use this file except in compliance with the License.<br>\n", "You may obtain a copy of the License at<br>\n", "<br>\n", "    http://www.apache.org/licenses/LICENSE-2.0<br>\n", "<br>\n", "Unless required by applicable law or agreed to in writing, software<br>\n", "distributed under the License is distributed on an \"AS IS\" BASIS,<br>\n", "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>\n", "See the License for the specific language governing permissions and<br>\n", "limitations under the License."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "from dataclasses import dataclass, field\n", "from typing import Optional"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import math\n", "import random\n", "import transformers\n", "from peft import PeftModel"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_attn_replace import replace_llama_attn\n", "from torch.distributed import init_process_group, destroy_process_group\n", "from torchmetrics import Accuracy\n", "from torchmetrics.text import Perplexity\n", "from torch.nn import CrossEntropyLoss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import inspect\n", "from abc import ABC, abstractmethod\n", "from typing import Union"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from torch.utils.data import Dataset, DataLoader, DistributedSampler\n", "from transformers.modeling_utils import PreTrainedModel\n", "from torch import nn\n", "from torch.nn.parallel import DistributedDataParallel as DDP\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import torch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Pg19Dataset(Dataset):\n", "    def __init__(self, data_path: str, seq_length: int, sliding_window: int = 256):\n", "        assert seq_length >= sliding_window, f\"Sliding window '{sliding_window}' must be smaller than sequence length '{seq_length}'\"\n", "        self.seq_length = seq_length\n", "        self.data = np.memmap(data_path, dtype=np.uint16, mode='r')\n", "        self.start_indices = list(range(0, len(self.data) - seq_length, sliding_window))\n", "        assert len(self) > 0, \"Dataset is empty\"\n", "    def __len__(self):\n", "        return len(self.start_indices)\n", "        # return 1000\n", "    def __getitem__(self, index) -> dict[str, torch.Tensor]:\n", "        start = self.start_indices[index]\n", "        end = start + self.seq_length\n", "        input_id = torch.from_numpy(self.data[start: end].astype(np.int64))\n", "        y = torch.from_numpy(self.data[start+1: end+1].astype(np.int64))\n", "        return {\n", "            \"input_ids\": input_id,\n", "            \"labels\": input_id,\n", "            \"ys\": y\n", "        }\n", "    def num_tokens(self):\n", "        return len(self.data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class EvalMetric(ABC):\n", "    @abstractmethod\n", "    def add(self, logits: torch.FloatTensor, labels: torch.LongTensor, model_output: object) -> dict[str, object]:\n", "        pass\n", "    @abstractmethod\n", "    def compute(self) -> dict[str, object]:\n", "        pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DistributedEvaluator:\n", "    def __init__(self,\n", "                 model: Union[PreTrainedModel, nn.Module],\n", "                 batch_size: int,\n", "                 refresh_rate: int,\n", "                 gpu_id: int):\n", "        self.gpu_id = gpu_id\n", "        self.batch_size = batch_size\n", "        self.refresh_rate = refresh_rate\n", "        self.model = DDP(model, device_ids=[self.gpu_id])\n", "    def evaluate(self, dataset: Dataset, metric: EvalMetric) -> dict[str, object]:\n", "        data_loader = self._prepare_dataloader(dataset)\n", "        self.model.eval()\n", "        with torch.no_grad():\n", "            if self.is_first_device():\n", "                data_loader = tqdm(data_loader)\n", "            for i, example_dict in enumerate(data_loader):\n", "                sig = inspect.signature(self.model.forward)\n", "                used = set(list(sig.parameters.keys()) + [\"input_ids\", \"labels\"])\n", "                inputs = {key: example_dict[key].to(self.gpu_id) for key in used if key in example_dict}\n", "                outputs = self.model(**inputs)\n", "                metric_result = metric.add(logits=outputs[\"logits\"], labels=inputs[\"labels\"], model_output=outputs)\n", "                if self.is_first_device() and (i % self.refresh_rate == 0):\n", "                    data_loader.set_postfix(metric_result)\n", "            return metric.compute()\n", "    def is_first_device(self):\n", "        return self.gpu_id == 0\n", "    def _prepare_dataloader(self, dataset: Dataset):\n", "        return DataLoader(\n", "            dataset,\n", "            batch_size=self.batch_size,\n", "            pin_memory=True,\n", "            shuffle=False,\n", "            sampler=DistributedSampler(dataset)\n", "        )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class EvalMetricImpl(EvalMetric):\n", "    def __init__(self, vocab_size: int, gpu_id: int):\n", "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=vocab_size).to(gpu_id)\n", "        self.perplexity = Perplexity(ignore_index=CrossEntropyLoss().ignore_index).to(gpu_id)\n", "        self.last_loss = 0.0\n", "    def add(self, logits: torch.FloatTensor, labels: torch.LongTensor, model_output: object) -> dict[str, object]:\n", "        shift_predictions = logits.argmax(dim=-1)[..., :-1]\n", "        shift_labels = labels[..., 1:]\n", "        current_accuracy = self.accuracy.forward(preds=shift_predictions, target=shift_labels)\n", "        shift_logits = logits[..., :-1, :]\n", "        current_perplexity = self.perplexity.forward(preds=shift_logits, target=shift_labels)\n", "        self.last_loss = model_output[\"loss\"].item()\n", "        return {\n", "            \"accuracy\": current_accuracy.item(),\n", "            \"perplexity\": current_perplexity.item(),\n", "            \"loss\": self.last_loss\n", "        }\n", "    def compute(self) -> dict[str, object]:\n", "        current_accuracy = self.accuracy.compute()\n", "        current_perplexity = self.perplexity.compute()\n", "        return {\n", "            \"accuracy\": current_accuracy.item(),\n", "            \"perplexity\": current_perplexity.item(),\n", "            \"loss\": self.last_loss\n", "        }"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@dataclass\n", "class EvalArguments:\n", "    batch_size: int = field(\n", "        default=1,\n", "        metadata={\"help\": \"batch size.\"},\n", "    )\n", "    base_model: Optional[str] = field(default=\"meta-llama/Llama-2-7b-hf\")\n", "    seq_len: int = field(\n", "        default=2048,\n", "        metadata={\"help\": \"context length during evaluation.\"},\n", "    )\n", "    context_size: int = field(\n", "        default=-1,\n", "        metadata={\"help\": \"context size during fine-tuning.\"},\n", "    )\n", "    peft_model: Optional[str] = field(default=None)\n", "    flash_attn: bool = field(\n", "        default=True,\n", "        metadata={\"help\": \"Whether use flash attention.\"},\n", "    )\n", "    data_path: str = field(\n", "        default=\"./test.bin\",\n", "        metadata={\"help\": \"test data path\"},\n", "    )\n", "    cache_dir: Optional[str] = field(default=\"./.cache\")\n", "    progress_bar_fresh_rate: int = field(\n", "        default=10,\n", "        metadata={\"help\": \"progress bar metrics fresh rate.\"},\n", "    )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_eval(args: EvalArguments):\n", "    torch_dtype = torch.float16\n", "    seed = 2\n", "    torch.manual_seed(seed)\n", "    random.seed(seed)\n", "    np.random.seed(seed)\n", "    dataset = Pg19Dataset(args.data_path, seq_length=args.seq_len, sliding_window=256)\n", "    if args.flash_attn:\n", "        replace_llama_attn(use_flash_attn=True, use_full=True)\n\n", "    # Set RoPE scaling factor\n", "    config = transformers.AutoConfig.from_pretrained(\n", "        args.base_model,\n", "        cache_dir=args.cache_dir,\n", "        use_cache=False\n", "    )\n", "    context_size = args.context_size if args.context_size > 0 else args.seq_len\n", "    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)  # this value should be 4096 for LLaMA2 models\n", "    if orig_ctx_len and context_size > orig_ctx_len:\n", "        scaling_factor = float(math.ceil(context_size / orig_ctx_len))\n", "        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n\n", "    # Load model and tokenizer\n", "    model = transformers.AutoModelForCausalLM.from_pretrained(\n", "        args.base_model,\n", "        config=config,\n", "        cache_dir=args.cache_dir,\n", "        torch_dtype=torch_dtype)\n", "    model.resize_token_embeddings(32001)\n", "    if args.peft_model:\n", "        trainable_params = os.path.join(args.peft_model, \"trainable_params.bin\")\n", "        if os.path.isfile(trainable_params):\n", "            model.load_state_dict(torch.load(trainable_params, map_location=model.device), strict=False)\n", "        else:\n", "            raise ValueError(\"Trainable input embedding and normalization are required.\")\n", "        model = PeftModel.from_pretrained(\n", "            model,\n", "            args.peft_model,\n", "            torch_dtype=torch_dtype,\n", "            offload_folder=args.cache_dir,\n", "        )\n\n", "    # This is a hacky way to enable distributed evaluation. Otherwise, without any trainable parameters, we will not\n", "    # be able to use DistributedDataParallel, although we don't update any parameters during evaluation.\n", "    [p.requires_grad_() for n, p in model.named_parameters() if any([k in n for k in [\"lm_head\"]])]\n", "    gpu_id = int(os.environ[\"LOCAL_RANK\"])\n", "    model.to(gpu_id)\n", "    evaluator = DistributedEvaluator(\n", "        model=model,\n", "        batch_size=args.batch_size,\n", "        refresh_rate=args.progress_bar_fresh_rate,\n", "        gpu_id=gpu_id)\n", "    if evaluator.is_first_device():\n", "        print(\"data path\", args.data_path)\n", "        print(\"base model\", args.base_model)\n", "        print(\"peft model\", args.peft_model)\n", "        print(f\"Num validation tokens: {dataset.num_tokens()}, Num validation examples: {len(dataset)}\")\n", "    eval_metric = EvalMetricImpl(vocab_size=config.vocab_size, gpu_id=gpu_id)\n", "    result = evaluator.evaluate(dataset, eval_metric)\n", "    if evaluator.is_first_device():\n", "        print(result)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def ddp_setup():\n", "    init_process_group(backend=\"nccl\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main(cmd_args: list[str] = None):\n", "    ddp_setup()\n", "    parser = transformers.HfArgumentParser((EvalArguments, ))\n", "    args: EvalArguments = parser.parse_args_into_dataclasses(cmd_args)[0]\n", "    try:\n", "        run_eval(args)\n", "    finally:\n", "        destroy_process_group()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}