{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Written by Yukang Chen<br>\n", "Some code based on https://github.com/epfml/landmark-attention<br>\n", "<br>\n", "Licensed under the Apache License, Version 2.0 (the \"License\");<br>\n", "you may not use this file except in compliance with the License.<br>\n", "You may obtain a copy of the License at<br>\n", "<br>\n", "    http://www.apache.org/licenses/LICENSE-2.0<br>\n", "<br>\n", "Unless required by applicable law or agreed to in writing, software<br>\n", "distributed under the License is distributed on an \"AS IS\" BASIS,<br>\n", "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>\n", "See the License for the specific language governing permissions and<br>\n", "limitations under the License."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import math\n", "import torch\n", "import argparse\n", "import random\n", "import numpy as np\n", "from tqdm import tqdm\n", "import transformers\n", "from peft import PeftModel\n", "from llama_attn_replace import replace_llama_attn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def parse_config():\n", "    parser = argparse.ArgumentParser(description='arg parser')\n", "    parser.add_argument('--batch_size', type=int, default=32, help='batch size during inference')\n", "    parser.add_argument('--base_model', type=str, default=\"/data1/pretrained-models/llama-7b-hf\")\n", "    parser.add_argument('--cache_dir', type=str, default=\"./cache\")\n", "    parser.add_argument('--seq_len', type=int, default=2048, help='context length during evaluation')\n", "    parser.add_argument('--context_size', type=int, default=-1, help='context size during fine-tuning')\n", "    parser.add_argument('--peft_model', type=str, default=None, help='')\n", "    parser.add_argument('--flash_attn', type=bool, default=True, help='')\n", "    parser.add_argument('--data_path', type=str, default=\"./test.bin\", help='')\n", "    args = parser.parse_args()\n", "    return args"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_as_batch(data, seq_length, batch_size, device='cpu', sliding_window=256):\n", "    all_ix = list(range(0, len(data) - seq_length, sliding_window))\n", "    all_ix.pop()\n", "    for idx in range(0, len(all_ix), batch_size):\n", "        ix = all_ix[idx:idx+batch_size]\n", "        assert all([idx + seq_length + 1 <= len(data) for idx in ix])\n", "        x = torch.stack([torch.from_numpy((data[i:i+seq_length]).astype(np.int64)) for i in ix])\n", "        y = torch.stack([torch.from_numpy((data[i+1:i+1+seq_length]).astype(np.int64)) for i in ix])\n", "        if device != 'cpu':\n", "            x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n", "        yield x, y"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def iceildiv(x, y):\n", "    return (x + y - 1) // y"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate(model, data, batch_size, device, seq_length, sliding_window=256, use_cache=False):\n", "    stats = {}\n", "    model.eval()\n", "    loss_list_val, acc_list = [], []\n", "    loss_step_list_val = []\n", "    with torch.no_grad():\n", "        print(f\"Using seq length {seq_length}\")\n", "        torch.set_printoptions(sci_mode=False)\n", "        for idx, (x, y) in tqdm(\n", "            enumerate(\n", "                get_as_batch(\n", "                    data['val'], \n", "                    seq_length, \n", "                    batch_size, \n", "                    device=device,\n", "                    sliding_window=sliding_window\n", "                )\n", "            ),\n", "            total=iceildiv(\n", "                iceildiv(len(data['val']), sliding_window),\n", "                batch_size\n", "            )\n", "        ):\n", "            val_loss = 0.\n", "            acc = 0.\n", "            cnt = 0\n", "            for part_idx, i in enumerate(range(0, x.shape[1], seq_length)):\n", "                part_len = x[:, i:i + seq_length].shape[1]\n", "                outputs = model(\n", "                    input_ids=x[:, i:i + seq_length],\n", "                    labels=x[:, i:i+seq_length].contiguous(),\n", "                    use_cache=use_cache)\n", "                val_loss = outputs.loss * part_len + val_loss\n", "                acc = ((outputs.logits.argmax(-1) == y[:, i:i+seq_length]).float().sum()) + acc\n", "                cnt += part_len\n", "                while len(loss_step_list_val) <= part_idx:\n", "                    loss_step_list_val.append([])\n", "                loss_step_list_val[part_idx].append(outputs.loss.item())\n", "            val_loss /= cnt\n", "            acc /= cnt\n", "            \n", "            loss_list_val.append(val_loss.item())\n", "            acc_list.append(acc.item())\n", "    stats['val_acc'] = torch.as_tensor(acc_list).mean().item()\n", "    stats['val_loss'] = torch.as_tensor(loss_list_val).mean().item()\n", "    stats['val_perplexity'] = 2.71828 ** stats['val_loss']\n", "    stats['val_perplexity_per_chunk'] = torch.exp(torch.as_tensor(loss_step_list_val).mean(dim=1))\n", "    return stats"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main(args):\n", "    device = \"cuda:0\"\n", "    seed = 2\n", "    torch.cuda.set_device(device)\n", "    torch.manual_seed(seed)\n", "    random.seed(seed)\n", "    np.random.seed(seed)\n", "    data = {'val': np.memmap(args.data_path, dtype=np.uint16, mode='r')}\n", "    print(f\"Num validation tokens: {len(data['val'])}\")\n", "    print(\"data path\", args.data_path)\n", "    print(\"base model\", args.base_model)\n", "    print(\"peft model\", args.peft_model)\n", "    if args.flash_attn:\n", "        replace_llama_attn(use_flash_attn=True, use_full=True)\n\n", "    # Set RoPE scaling factor\n", "    config = transformers.AutoConfig.from_pretrained(\n", "        args.base_model,\n", "        cache_dir=args.cache_dir,\n", "    )\n", "    context_size = args.context_size if args.context_size > 0 else args.seq_len\n", "    orig_ctx_len = getattr(config, \"max_position_embeddings\", None) # this value should be 4096 for LLaMA2 models\n", "    if orig_ctx_len and context_size > orig_ctx_len:\n", "        scaling_factor = float(math.ceil(context_size / orig_ctx_len))\n", "        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n\n", "    # Load model and tokenizer\n", "    model = transformers.AutoModelForCausalLM.from_pretrained(\n", "        args.base_model,\n", "        config=config,\n", "        cache_dir=args.cache_dir,\n", "        torch_dtype=torch.float16,\n", "        device_map=\"auto\",\n", "    )\n", "    model.resize_token_embeddings(32001)\n", "    if args.peft_model:\n", "        trainable_params = os.path.join(args.peft_model, \"trainable_params.bin\")\n", "        if os.path.isfile(trainable_params):\n", "            model.load_state_dict(torch.load(trainable_params, map_location=model.device), strict=False)\n", "        else:\n", "            raise ValueError(\"Trainable input embedding and normalization are required.\")\n", "        model = PeftModel.from_pretrained(\n", "            model,\n", "            args.peft_model,\n", "            device_map=\"auto\",\n", "            torch_dtype=torch.float16,\n", "        )\n", "    stats = evaluate(model, data, args.batch_size, device, args.seq_len, sliding_window=256)\n", "    print(stats)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    args = parse_config()\n", "    main(args)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}