{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Written by Yukang Chen<br>\n", "Some code based on https://github.com/epfml/landmark-attention<br>\n", "<br>\n", "Licensed under the Apache License, Version 2.0 (the \"License\");<br>\n", "you may not use this file except in compliance with the License.<br>\n", "You may obtain a copy of the License at<br>\n", "<br>\n", "    http://www.apache.org/licenses/LICENSE-2.0<br>\n", "<br>\n", "Unless required by applicable law or agreed to in writing, software<br>\n", "distributed under the License is distributed on an \"AS IS\" BASIS,<br>\n", "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>\n", "See the License for the specific language governing permissions and<br>\n", "limitations under the License."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import math\n", "from dataclasses import dataclass, field\n", "from functools import partial\n", "from typing import Dict, Optional, Sequence"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import transformers\n", "from torch.utils.data import Dataset\n", "from transformers import Trainer, DataCollatorForLanguageModeling\n", "from llama_attn_replace import replace_llama_attn\n", "from gptneox_attn_replace import replace_gpt_neox_attn\n", "from peft import LoraConfig, get_peft_model\n", "from torch.distributed import barrier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datasets import load_dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["IGNORE_INDEX = -100\n", "DEFAULT_PAD_TOKEN = \"[PAD]\"\n", "DEFAULT_EOS_TOKEN = \"</s>\"\n", "DEFAULT_BOS_TOKEN = \"<s>\"\n", "DEFAULT_UNK_TOKEN = \"<unk>\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@dataclass\n", "class ModelArguments:\n", "    model_name_or_path: Optional[str] = field(default=\"EleutherAI/pythia-1.4b-deduped\")\n", "    model_type: Optional[str] = field(default=\"llama\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@dataclass\n", "class TrainingArguments(transformers.TrainingArguments):\n", "    cache_dir: Optional[str] = field(default=None)\n", "    optim: str = field(default=\"adamw_torch\")\n", "    model_max_length: int = field(\n", "        default=8192 * 4,\n", "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n", "    )\n", "    use_flash_attn: bool = field(\n", "        default=True,\n", "        metadata={\"help\": \"Whether use flash attention for training.\"},\n", "    )\n", "    use_full_attn: bool = field(\n", "        default=False,\n", "        metadata={\"help\": \"Whether to use plain, full-attention for training.\"},\n", "    )\n", "    low_rank_training: bool = field(\n", "        default=True,\n", "        metadata={\"help\": \"Whether use low rank adaptation for training.\"},\n", "    )\n", "    trainable_params: str = field(\n", "        default=\"embed,norm\",\n", "        metadata={\"help\": \"Additional trainable parameters except LoRA weights, if low rank training.\"},\n", "    )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def smart_tokenizer_and_embedding_resize(\n", "    special_tokens_dict: Dict,\n", "    tokenizer: transformers.PreTrainedTokenizer,\n", "    model: transformers.PreTrainedModel,\n", "):\n", "    \"\"\"Resize tokenizer and embedding.\n", "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n", "    \"\"\"\n", "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n", "    model.resize_token_embeddings(len(tokenizer))\n", "    if num_new_tokens > 0:\n", "        input_embeddings = model.get_input_embeddings().weight.data\n", "        output_embeddings = model.get_output_embeddings().weight.data\n", "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n", "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n", "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n", "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tokenize_fn(tokenizer, example):\n", "    context_length = tokenizer.model_max_length\n", "    outputs = tokenizer(\n", "        tokenizer.eos_token.join(example[\"text\"]),\n", "        truncation=False,\n", "        return_tensors=\"pt\",\n", "        pad_to_multiple_of=context_length,\n", "        padding=True,\n", "    )\n", "    return {\"input_ids\": outputs[\"input_ids\"].view(-1, context_length)}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train():\n", "    parser = transformers.HfArgumentParser((ModelArguments, TrainingArguments))\n", "    model_args, training_args = parser.parse_args_into_dataclasses()\n\n", "    # NOTE: May expand supported model types in the future\n", "    if model_args.model_type == \"gpt-neox\":\n", "        replace_gpt_neox_attn(training_args.use_flash_attn, training_args.use_full_attn)\n", "    else:\n", "        assert model_args.model_type == \"llama\", \"Only support llama and gpt-neox for now\"\n", "        replace_llama_attn(training_args.use_flash_attn, training_args.use_full_attn)\n\n", "    # Set RoPE scaling factor\n", "    config = transformers.AutoConfig.from_pretrained(\n", "        model_args.model_name_or_path,\n", "        cache_dir=training_args.cache_dir,\n", "    )\n", "    orig_rope_scaling = getattr(config, \"rope_scaling\", None)\n", "    if orig_rope_scaling is None:\n", "        orig_rope_scaling = {\"factor\": 1}\n", "    orig_rope_scaling_factor = orig_rope_scaling[\"factor\"] if \"factor\" in orig_rope_scaling.keys() else 1\n", "    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n", "    if orig_ctx_len:\n", "        orig_ctx_len *= orig_rope_scaling_factor\n", "        if training_args.model_max_length > orig_ctx_len:\n", "            scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n", "            config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n\n", "    # Load model and tokenizer\n", "    model = transformers.AutoModelForCausalLM.from_pretrained(\n", "        model_args.model_name_or_path,\n", "        config=config,\n", "        cache_dir=training_args.cache_dir,\n", "        torch_dtype=torch.bfloat16,\n", "    )\n", "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n", "        model_args.model_name_or_path,\n", "        cache_dir=training_args.cache_dir,\n", "        model_max_length=training_args.model_max_length,\n", "        padding_side=\"right\",\n", "        use_fast=True,\n", "    )\n", "    special_tokens_dict = dict()\n", "    if tokenizer.pad_token is None:\n", "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n", "    if tokenizer.eos_token is None:\n", "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n", "    if tokenizer.bos_token is None:\n", "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n", "    if tokenizer.unk_token is None:\n", "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n", "    smart_tokenizer_and_embedding_resize(\n", "        special_tokens_dict=special_tokens_dict,\n", "        tokenizer=tokenizer,\n", "        model=model,\n", "    )\n", "    rank = int(os.environ.get('RANK', -1))\n", "    if rank > 0:\n", "        barrier()\n", "    dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", cache_dir=training_args.cache_dir)\n", "    dataset = dataset.map(partial(tokenize_fn,tokenizer),batched=True, num_proc=128, remove_columns=[\"text\", \"meta\"])\n", "    if rank == 0:\n", "        barrier()\n", "    print(dataset)\n", "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n", "    if training_args.low_rank_training:\n", "        if model_args.model_type == \"gpt-neox\":\n", "            # added `dense` to match with llama as the basic LoRA would only target 'query_key_value'\n", "            targets = [\"query_key_value\", \"dense\"]\n", "        else:\n", "            targets=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n", "        config = LoraConfig(\n", "            r=8,\n", "            lora_alpha=16,\n", "            target_modules=targets,\n", "            lora_dropout=0,\n", "            bias=\"none\",\n", "            task_type=\"CAUSAL_LM\",\n", "        )\n", "        model = get_peft_model(model, config)\n", "        # enable trainable params\n", "        [p.requires_grad_() for n, p in model.named_parameters() if any([k in n for k in training_args.trainable_params.split(\",\")])]\n", "    model.config.use_cache = False         # required for gradient checkpointing\n", "    model.enable_input_require_grads()     # required for gradient checkpointing\n", "    model.gradient_checkpointing_enable()  # enable gradient checkpointing\n", "    trainer = Trainer(\n", "        model=model, tokenizer=tokenizer, args=training_args,\n", "        train_dataset=dataset[\"train\"],\n", "        eval_dataset=None,\n", "        data_collator=data_collator)\n", "    trainer.train()\n", "    trainer.save_state()\n", "    trainer.save_model(output_dir=training_args.output_dir)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    train()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}