{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Written by Yukang Chen<br>\n", "<br>\n", "Licensed under the Apache License, Version 2.0 (the \"License\");<br>\n", "you may not use this file except in compliance with the License.<br>\n", "You may obtain a copy of the License at<br>\n", "<br>\n", "    http://www.apache.org/licenses/LICENSE-2.0<br>\n", "<br>\n", "Unless required by applicable law or agreed to in writing, software<br>\n", "distributed under the License is distributed on an \"AS IS\" BASIS,<br>\n", "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>\n", "See the License for the specific language governing permissions and<br>\n", "limitations under the License."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import torch\n", "import argparse\n", "import transformers\n", "from peft import PeftModel\n", "from typing import Dict"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["IGNORE_INDEX = -100\n", "DEFAULT_PAD_TOKEN = \"[PAD]\"\n", "DEFAULT_EOS_TOKEN = \"</s>\"\n", "DEFAULT_BOS_TOKEN = \"<s>\"\n", "DEFAULT_UNK_TOKEN = \"<unk>\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def parse_config():\n", "    parser = argparse.ArgumentParser(description='arg parser')\n", "    parser.add_argument('--base_model', type=str, default=\"/data/pretrained-models/llama-7b-hf\")\n", "    parser.add_argument('--peft_model', type=str, default=None, help='')\n", "    parser.add_argument('--context_size', type=int, default=-1, help='context size during fine-tuning')\n", "    parser.add_argument('--save_path', type=str, default=None, help='')\n", "    parser.add_argument('--cache_dir', type=str, default=None, help='./cache_dir')\n", "    args = parser.parse_args()\n", "    return args"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def smart_tokenizer_and_embedding_resize(\n", "    special_tokens_dict: Dict,\n", "    tokenizer: transformers.PreTrainedTokenizer,\n", "    model: transformers.PreTrainedModel,\n", "):\n", "    \"\"\"Resize tokenizer and embedding.\n", "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n", "    \"\"\"\n", "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n", "    model.resize_token_embeddings(len(tokenizer))\n", "    if num_new_tokens > 0:\n", "        input_embeddings = model.get_input_embeddings().weight.data\n", "        output_embeddings = model.get_output_embeddings().weight.data\n", "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n", "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n", "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n", "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main(args):\n", "    device = \"cuda:0\"\n", "    torch.cuda.set_device(device)\n", "    print(\"base model\", args.base_model)\n", "    print(\"peft model\", args.peft_model)\n\n", "    # Load model and tokenizer\n", "    model = transformers.AutoModelForCausalLM.from_pretrained(\n", "        args.base_model,\n", "        cache_dir=args.cache_dir,\n", "        torch_dtype=torch.float16,\n", "        device_map=\"auto\",\n", "    )\n", "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n", "        args.base_model,\n", "        cache_dir=args.cache_dir,\n", "        model_max_length=args.context_size,\n", "        padding_side=\"right\",\n", "        use_fast=False,\n", "    )\n", "    special_tokens_dict = dict()\n", "    if tokenizer.pad_token is None:\n", "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n", "    if tokenizer.eos_token is None:\n", "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n", "    if tokenizer.bos_token is None:\n", "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n", "    if tokenizer.unk_token is None:\n", "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n", "    smart_tokenizer_and_embedding_resize(\n", "        special_tokens_dict=special_tokens_dict,\n", "        tokenizer=tokenizer,\n", "        model=model,\n", "    )\n", "    trainable_params = os.path.join(args.peft_model, \"trainable_params.bin\")\n", "    if os.path.isfile(trainable_params):\n", "        model.load_state_dict(torch.load(trainable_params, map_location=model.device), strict=False)\n", "    model = PeftModel.from_pretrained(\n", "        model,\n", "        args.peft_model,\n", "        device_map=\"auto\",\n", "        torch_dtype=torch.float16,\n", "    )\n", "    model = model.merge_and_unload()\n", "    model.save_pretrained(args.save_path)\n", "    tokenizer.save_pretrained(args.save_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    args = parse_config()\n", "    main(args)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}