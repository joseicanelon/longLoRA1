{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Modified based on https://github.com/dvlab-research/LongLoRA"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from typing import Optional, Tuple\n", "import warnings\n", "import torch\n", "import transformers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from einops import rearrange\n", "from flash_attn import flash_attn_varlen_qkvpacked_func, flash_attn_varlen_func\n", "from flash_attn.bert_padding import unpad_input, pad_input"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["group_size_ratio = 1/4"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def rotate_half(x):\n", "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n", "    x1 = x[..., : x.shape[-1] // 2]\n", "    x2 = x[..., x.shape[-1] // 2 :]\n", "    return torch.cat((-x2, x1), dim=-1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n", "    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n", "    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n", "    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1).to(q.dtype), 2, gather_indices)\n", "    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1).to(k.dtype), 2, gather_indices)\n", "    q_embed = (q * cos) + (rotate_half(q) * sin)\n", "    k_embed = (k * cos) + (rotate_half(k) * sin)\n", "    return q_embed, k_embed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def _flash_attn_ssa(query, key, value, attention_mask=None, head_mask=None):\n", "    # transform the data into the qkv packed form\n", "    qkv = torch.stack(\n", "        [query, key, value], dim=2\n", "    ) # [bsz, nh, 3, q_len, hd]\n", "    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n", "    bsz, q_len = qkv.shape[:2]\n", "    qkv = rearrange(qkv, \"b s ... -> (b s) ...\")\n", "    cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32, device=qkv.device)\n", "    output = flash_attn_varlen_qkvpacked_func(qkv, cu_q_lens, q_len, 0.0, softmax_scale=None, causal=True)\n", "    output = rearrange(output, \"(b s) ... -> b s ...\", b=bsz)\n\n", "    # disable attn weights by returning None when using flash attention\n", "    return output, None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def _flash_attn_full(query, key, value, attention_mask=None, head_mask=None):\n", "    # q, k, v: [bs, nh, seq_len, hd]\n", "    batch_size, num_attention_heads, query_length, attn_head_size = query.size()\n", "    key_length = key.size(-2)\n", "    value_length = value.size(-2)\n\n", "    # q, k, v: [bs, nh, seq_len, hd] -> [bs, seq_len, nh, hd] -> [bs * seq_len, nh, hd]\n", "    query = query.transpose(1, 2).reshape(batch_size * query_length , num_attention_heads, attn_head_size)\n", "    key = key.transpose(1, 2).reshape(batch_size * key_length, num_attention_heads, attn_head_size)\n", "    value = value.transpose(1, 2).reshape(batch_size * value_length, num_attention_heads, attn_head_size)\n", "    cu_seqlens_q = torch.arange(\n", "        0,\n", "        (batch_size + 1) * query_length,\n", "        step=query_length,\n", "        dtype=torch.int32,\n", "        device=query.device,\n", "    )\n", "    cu_seqlens_k = torch.arange(\n", "        0,\n", "        (batch_size + 1) * key_length,\n", "        step=key_length,\n", "        dtype=torch.int32,\n", "        device=key.device,\n", "    )\n", "    attn_output, attn_weights, _ = flash_attn_varlen_func(\n", "        query, key, value, cu_seqlens_q, cu_seqlens_k, query_length, value_length, dropout_p=0.0,\n", "        softmax_scale=None, causal=True, return_attn_probs=True\n", "    )\n", "    attn_output = attn_output.view(batch_size, query_length, num_attention_heads, attn_head_size).transpose(1, 2)\n", "    return attn_output, attn_weights"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_forward_function(use_flash_attn=True, use_full=False):\n", "    def forward_attention(\n", "        self,\n", "        hidden_states: torch.FloatTensor,\n", "        attention_mask: torch.FloatTensor,\n", "        position_ids: torch.LongTensor,\n", "        head_mask: Optional[torch.FloatTensor] = None,\n", "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n", "        use_cache: Optional[bool] = False,\n", "        output_attentions: Optional[bool] = False,\n", "    ):\n", "        # NOTE: compute SS group size\n", "        bsz, q_len, _ = hidden_states.size()\n", "        has_layer_past = layer_past is not None\n\n", "        # Compute QKV\n", "        # Attention heads [batch, seq_len, hidden_size]\n", "        #   --> [batch, seq_len, (np * 3 * head_size)]\n", "        qkv = self.query_key_value(hidden_states)\n\n", "        # [batch, seq_len, (num_heads * 3 * head_size)]\n", "        #   --> [batch, seq_len, num_heads, 3 * head_size]\n", "        new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n", "        qkv = qkv.view(*new_qkv_shape)\n\n", "        # [batch, seq_len, num_attention_heads, 3 * head_size]\n", "        #   --> 3 [batch, num_attention_heads, seq_len, head_size]\n", "        query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n", "        key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n", "        value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n", "        # [bsz, nh, q_len, hd]\n\n", "        # Compute rotary embeddings on rotary_ndims\n", "        query_rot = query[..., : self.rotary_ndims]\n", "        query_pass = query[..., self.rotary_ndims :]\n", "        key_rot = key[..., : self.rotary_ndims]\n", "        key_pass = key[..., self.rotary_ndims :]\n\n", "        # Compute token offset for rotary embeddings (when decoding)\n", "        seq_len = key.shape[-2]\n", "        if has_layer_past:\n", "            seq_len += layer_past[0].shape[-2]\n", "        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n", "        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n", "        query = torch.cat((query, query_pass), dim=-1)\n", "        key = torch.cat((key, key_pass), dim=-1)\n\n", "        # Cache QKV values\n", "        if has_layer_past:\n", "            past_key = layer_past[0]\n", "            past_value = layer_past[1]\n", "            key = torch.cat((past_key, key), dim=-2)\n", "            value = torch.cat((past_value, value), dim=-2)\n", "        present = (key, value) if use_cache else None\n\n", "        # NOTE: apply shift\n", "        group_size = int(q_len * group_size_ratio)\n", "        if q_len % group_size > 0:\n", "            raise ValueError(\"q_len %d should be divisible by group size %d.\" % (q_len, group_size))\n", "        num_group = q_len // group_size\n", "        if self.training and not use_full:\n", "            def shift(qkv, num_heads, head_dim):\n", "                # qkv = [bsz, nh, q_len, d]\n", "                qkv = qkv.transpose(1, 2)\n", "                # qkv = [bsz, q_len, nh, d]\n", "                qkv[:, :, num_heads//2:] = qkv[:, :, num_heads//2:].roll(-group_size//2, dims=1)\n", "                # -> [bsz * n_group, group_s, nh, d)\n", "                #   -> [bsz * n_group, nh, group_s, d)\n", "                qkv = qkv.reshape(bsz * num_group, group_size, num_heads, head_dim).transpose(1, 2)\n", "                return qkv\n\n", "            # contiguous is required as self._attn() will attempt to apply .view() on them\n", "            query = shift(query, self.num_attention_heads, self.head_size).contiguous()\n", "            key = shift(key, self.num_attention_heads, self.head_size).contiguous()\n", "            value = shift(value, self.num_attention_heads, self.head_size).contiguous()\n", "            attention_mask = attention_mask[:, :, :group_size, :group_size].repeat(num_group, 1, 1, 1)\n\n", "        # Compute attention\n", "        if use_flash_attn:\n", "            _flash_attn = _flash_attn_full if use_full else _flash_attn_ssa\n", "            attn_output, attn_weights = _flash_attn(query, key, value, attention_mask, head_mask)\n", "        else:\n", "            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n", "        # NOTE: shift back\n", "        if self.training and not use_full:\n", "            attn_output = attn_output.transpose(1, 2).contiguous()\n", "            attn_output = attn_output.reshape(bsz, q_len, self.num_attention_heads, self.head_size)\n", "            # [bsz, q_len, nh, hd]\n", "            attn_output[:, :, self.num_attention_heads//2:] = attn_output[:, :, self.num_attention_heads//2:].roll(group_size//2, dims=1)\n", "            attn_output = attn_output.transpose(1, 2)\n\n", "        # Reshape outputs\n", "        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n", "        attn_output = self.dense(attn_output)\n", "        outputs = (attn_output, present)\n", "        if output_attentions:\n", "            outputs += (attn_weights,)\n", "        return outputs\n", "    return forward_attention"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def replace_gpt_neox_attn(use_flash_attn=True, use_full=False):\n", "    cuda_major, cuda_minor = torch.cuda.get_device_capability()\n", "    if use_flash_attn and cuda_major < 8:\n", "        warnings.warn(\n", "            \"Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.\"\n", "            \"ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\"\n", "            \"Resorting to plain attention...\"\n", "        )\n", "        use_flash_attn = False\n", "    forward_fn = get_forward_function(use_flash_attn, use_full)\n", "    transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.forward = forward_fn"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}