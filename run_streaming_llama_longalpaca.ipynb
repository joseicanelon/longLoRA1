{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["warnings.filterwarnings(\"ignore\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import argparse\n", "import json\n", "import os\n", "import time\n", "import re\n", "import sys"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tqdm import tqdm\n", "from streaming_llm.utils import load, download_url, load_jsonl\n", "from streaming_llm.enable_streaming_llm import enable_streaming_llm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@torch.no_grad()\n", "def greedy_generate(model, tokenizer, input_ids, past_key_values, max_gen_len):\n", "    outputs = model(\n", "        input_ids=input_ids,\n", "        past_key_values=past_key_values,\n", "        use_cache=True,\n", "    )\n", "    past_key_values = outputs.past_key_values\n", "    pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n", "    generated_ids = [pred_token_idx.item()]\n", "    pos = 0\n", "    for _ in range(max_gen_len - 1):\n", "        outputs = model(\n", "            input_ids=pred_token_idx,\n", "            past_key_values=past_key_values,\n", "            use_cache=True,\n", "        )\n", "        past_key_values = outputs.past_key_values\n", "        pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n", "        generated_ids.append(pred_token_idx.item())\n", "        generated_text = (\n", "            tokenizer.decode(\n", "                generated_ids,\n", "                skip_special_tokens=True,\n", "                clean_up_tokenization_spaces=True,\n", "                spaces_between_special_tokens=False,\n", "            )\n", "            .strip()\n", "            .split(\" \")\n", "        )\n", "        now = len(generated_text) - 1\n", "        if now > pos:\n", "            print(\" \".join(generated_text[pos:now]), end=\" \", flush=True)\n", "            pos = now\n", "        if pred_token_idx == tokenizer.eos_token_id:\n", "            break\n", "    print(\" \".join(generated_text[pos:]), flush=True)\n", "    return past_key_values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@torch.no_grad()\n", "def streaming_inference(model, tokenizer, prompts, kv_cache=None, max_gen_len=1000):\n", "    past_key_values = None\n", "    for idx, prompt in enumerate(prompts):\n", "        prompt = \"USER: \" + prompt + \"\\n\\nASSISTANT: \"\n", "        print(\"\\n\" + prompt, end=\"\")\n", "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n", "        input_ids = input_ids.to(model.device)\n", "        seq_len = input_ids.shape[1]\n", "        if kv_cache is not None:\n", "            space_needed = seq_len + max_gen_len\n", "            past_key_values = kv_cache.evict_for_space(past_key_values, space_needed)\n", "        past_key_values = greedy_generate(\n", "            model, tokenizer, input_ids, past_key_values, max_gen_len=max_gen_len\n", "        )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main(args):\n", "    model_name_or_path = args.model_name_or_path\n", "    model, tokenizer = load(model_name_or_path)\n", "    print(f\"Loading data from {args.test_filepath} ...\")\n", "    list_data = json.load(open(args.test_filepath))\n", "    prompts = []\n", "    for sample in list_data:\n", "        prompts += [sample[\"instruction\"]]\n", "    if args.enable_streaming:\n", "        kv_cache = enable_streaming_llm(\n", "            model, start_size=args.start_size, recent_size=args.recent_size, use_flash_attn=args.use_flash_attn\n", "        )\n", "    else:\n", "        kv_cache = None\n", "    streaming_inference(\n", "        model,\n", "        tokenizer,\n", "        prompts,\n", "        kv_cache,\n", "    )"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}